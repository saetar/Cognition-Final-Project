{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### May "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### May 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally got output of the form we want. Ran our code on `mn.txt`, a list of tweets relating to Minnesota. Fixed bugs relating to our `document.py` module that deals with our distribution over words and topics. \n",
    "\n",
    "We implemented stop words for our model to exclude when analyzing our documents. Added additional filters for urls and any non-alphabetic strings in our tweets, including hashtags. \n",
    "\n",
    "Trained our model for the first time, using 10 iterations of improving topics. \n",
    "\n",
    "It is taking a very long time to run this on `all_tweets`, but we are hoping to see topics that relate somewhat to the list of documents given, which are college, mn, my life, obama, party, sandwich, sentiments, and soup. We will have to wait to see how successful we are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### May 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that our model was not working with the actual Dirichlet distribution so we looked at how we could implement the multinomial and the Dirichlet distribution as discussed in class.\n",
    "\n",
    "We learned that $\\alpha$ and $\\beta$ are only important for the initialization of our distributions. $\\alpha$ seems to be the initial value of each entry in our (document,topic) distribution matrix. $\\beta \\cdot V$ where $V$ is the number of distinct words in our corpus was the inital value for each entry in our (topic, word) distribution matrix.\n",
    "\n",
    "We learned that we stil needed to use the multinomial when reassigning topics within documents.\n",
    "\n",
    "We talked to Anna about Twitter API which was very helpful in the end as we got an actual `curl` command that gave back something that we wanted. Thanks for that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### May 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented multinomial distribution.\n",
    "\n",
    "Still uses dumbinit\n",
    "\n",
    "Model runs a lot faster because it uses `numpy` arrays and an overall matrix structure as opposed to heirarchical object-oriented-ness which was causing a lot of issues with the interconnectivity between a lot of variables. \n",
    "\n",
    "We know have 3 overall references in the `LDA` class:\n",
    "1. `doc_topic_counts` is a matrix of size (documents x topics) and keeps track of the counts of each topic within each document. \n",
    "2. `topics` is a matrix of size (topics x vocab) where vocab is a list of all unique words in the corpus.\n",
    "3. `topic_word_counts` is an array of size (topics) and keeps a count of the number of words assigned to each topic. \n",
    "    \n",
    "Within each document, we still have a list where each index stands for the topic that that word was assigned. This is stored in our `Document` class, not in `LDA`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
